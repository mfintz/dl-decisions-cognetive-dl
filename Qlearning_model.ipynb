{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import os\n",
    "np.random.seed(7)\n",
    "from scipy.spatial import distance\n",
    "from numpy.random import choice\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook to recreate the qlearning part, from optimization to plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.use('svg')\n",
    "new_rc_params = {\n",
    "    \"font.size\": 20, #choosing the font size helps latex to place all the labels, ticks etc. in the right place\n",
    "    \"svg.fonttype\": 'none',\n",
    "    'figure.max_open_warning': 0\n",
    "} \n",
    "mpl.rcParams.update(new_rc_params)\n",
    "#     \"font.weight\":700\n",
    "plt.rcParams['axes.facecolor'] = '0.95'\n",
    "\n",
    "my_font_dict = {\n",
    "        'size': 20,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_dir = os.path.join(os.getcwd(), 'paper_additions')\n",
    "if not os.path.exists(saving_dir):\n",
    "    os.makedirs(saving_dir)\n",
    "    \n",
    "data = pd.read_csv('DataAllSubjectsRewards.csv')\n",
    "data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load prior results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_saving_dir = \"cross_validation/averages/general/\"\n",
    "if not os.path.exists(general_saving_dir):\n",
    "    os.makedirs(general_saving_dir)\n",
    "    \n",
    "reward_oriented_saving_dir = \"cross_validation/averages/reward_oriented/\"\n",
    "if not os.path.exists(reward_oriented_saving_dir):\n",
    "    os.makedirs(reward_oriented_saving_dir)\n",
    "    \n",
    "no_reward_saving_dir = \"cross_validation/averages/no_reward/\"\n",
    "if not os.path.exists(no_reward_saving_dir):\n",
    "    os.makedirs(no_reward_saving_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "with open(os.path.join(general_saving_dir, 'general_all_folds_all_payoffs_true_false.pkl'), 'rb') as f:\n",
    "    general_all_folds_all_payoffs_true_false = pickle.load(f)\n",
    "general_all_folds_true_false, general_payoff2_persons_true_false_predictions_lstm, general_payoff3_persons_true_false_predictions_lstm, general_payoff4_persons_true_false_predictions_lstm = general_all_folds_all_payoffs_true_false\n",
    "\n",
    "with open(os.path.join(general_saving_dir, 'general_all_accuracy_per_par.pkl'), 'rb') as f:\n",
    "    general_all_accuracy_per_par = pickle.load(f)\n",
    "general_all_folds_lstm_accs, general_payoff2_lstm_accs, general_payoff3_lstm_accs, general_payoff4_lstm_accs = general_all_accuracy_per_par\n",
    "\n",
    "\n",
    "# Reward-Oblivious\n",
    "with open(os.path.join(no_reward_saving_dir, 'no_reward_all_folds_all_payoffs_true_false.pkl'), 'rb') as f:\n",
    "    no_reward_all_folds_all_payoffs_true_false = pickle.load(f)\n",
    "no_reward_all_folds_true_false, no_reward_payoff2_persons_true_false_predictions_lstm, no_reward_payoff3_persons_true_false_predictions_lstm, no_reward_payoff4_persons_true_false_predictions_lstm = no_reward_all_folds_all_payoffs_true_false\n",
    "\n",
    "with open(os.path.join(no_reward_saving_dir, 'no_reward_all_accuracy_per_par.pkl'), 'rb') as f:\n",
    "    no_reward_all_accuracy_per_par = pickle.load(f)\n",
    "no_reward_all_folds_lstm_accs, no_reward_payoff2_lstm_accs, no_reward_payoff3_lstm_accs, no_reward_payoff4_lstm_accs = no_reward_all_accuracy_per_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized qlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filled_NA = data\n",
    "# data_noNA = full_orig_data.dropna()\n",
    "# notice that here is where I choose if feeding full data with blanks, or without blanks\n",
    "# without blanks:\n",
    "# data_payoff2 = data_noNA[data_noNA['payoff_structure']==2]\n",
    "# data_payoff3 = data_noNA[data_noNA['payoff_structure']==3]\n",
    "# data_payoff4 = data_noNA[data_noNA['payoff_structure']==4]\n",
    "# with blanks:\n",
    "data_payoff2 = data_filled_NA[data_filled_NA['payoff_structure']==2]\n",
    "data_payoff3 = data_filled_NA[data_filled_NA['payoff_structure']==3]\n",
    "data_payoff4 = data_filled_NA[data_filled_NA['payoff_structure']==4]\n",
    "user_ids2 = data_payoff2.user.unique()\n",
    "user_ids3 = data_payoff3.user.unique()\n",
    "user_ids4 = data_payoff4.user.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stickiness(person_df):\n",
    "    \"\"\"\n",
    "    probability of switch - count how many times the person pressed the same choice again (first doesn't count)\n",
    "    \"\"\"\n",
    "    sticky_count = 0\n",
    "    for i,choice in enumerate(person_df.choice):\n",
    "        if i==0:\n",
    "            continue\n",
    "        if (person_df.choice[i] == person_df.choice[i-1]):\n",
    "            sticky_count+=1\n",
    "    return sticky_count/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_reward(person_df):\n",
    "    return np.mean(person_df.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed version\n",
    "# 30.10.21 paper addition - data needs to be full payoff\n",
    "def q_learning_model(params, *args):\n",
    "    choices = list(args[0].choice.astype(int))\n",
    "    rewards = list(np.divide(list(args[0].reward.astype(int)),98))\n",
    "    users = list(args[0].user.astype(int))\n",
    "    num_of_trials = len(choices)\n",
    "\n",
    "    prob_of_choice = [0]*num_of_trials\n",
    "    accuracy = [0]*num_of_trials\n",
    "\n",
    "    # notice: I want to init q to this for every new participant\n",
    "    q = list(np.divide([50,50,50,50],98))\n",
    "    init_q = list(np.divide([50,50,50,50],98))\n",
    "    beta = params[0]\n",
    "    alpha = params[1]\n",
    "\n",
    "\n",
    "    q_predictions = []\n",
    "\n",
    "    last_user = users[0]\n",
    "    for trial in range(num_of_trials):\n",
    "        if users[trial] != last_user:\n",
    "            # print(f'in fitting, finished user {last_user} changing to next, init q values')\n",
    "            last_user = users[trial]\n",
    "            q = init_q\n",
    "        if choices[trial] == -1:\n",
    "            # if it's first trial, there's no previous prediction to count on - make a random choice then. Reward will be 0\n",
    "            if trial == 0:\n",
    "                random_q_ind = random.choice([0,1,2,3])\n",
    "                prob_of_choice[trial] = np.divide( np.exp(np.multiply(beta, q[random_q_ind])),  np.sum(np.exp(np.multiply(beta,q)), axis=0))\n",
    "                accuracy[trial] = int((choices[trial]-1)==random_q_ind)\n",
    "                q_predictions.append(random_q_ind)\n",
    "                # pe = 0 - q[random_q_ind]\n",
    "                # q[random_q_ind] = q[random_q_ind] + alpha*pe\n",
    "            else:\n",
    "                prob_of_choice[trial] =  prob_of_choice[trial-1]\n",
    "                accuracy[trial] = accuracy[trial]-1\n",
    "                q_predictions.append(q_predictions[-1])\n",
    "                # pe = 0 - q[q_predictions[-1]]\n",
    "                # q[q_predictions[-1]] = q[q_predictions[-1]] + alpha*pe\n",
    "        else:\n",
    "            # regular choice case\n",
    "            prob_of_choice[trial] = np.divide( np.exp(np.multiply(beta, q[choices[trial]-1])),  np.sum(np.exp(np.multiply(beta,q)), axis=0))\n",
    "            accuracy[trial] = int((choices[trial]-1)==np.argmax(q))\n",
    "            q_predictions.append(np.argmax(q))\n",
    "            pe = rewards[trial]-q[choices[trial]-1]\n",
    "            q[choices[trial]-1] = q[choices[trial]-1] + alpha*pe\n",
    "        # prob_of_choice[trial] = np.divide( np.exp(np.multiply(beta, q[choices[trial]-1])),  np.sum(np.exp(np.multiply(beta,q)), axis=0))\n",
    "\n",
    "        # pe = rewards[trial]-q[choices[trial]-1]\n",
    "        # q[choices[trial]-1] = q[choices[trial]-1] + alpha*pe\n",
    "        \n",
    "\n",
    "    return -(np.sum(np.log(prob_of_choice)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed version\n",
    "# 30.10.21 paper addition - data needs to be full payoff\n",
    "def q_learning_model_fitted(params, data):\n",
    "    beta = params.x[0]\n",
    "    alpha = params.x[1]\n",
    "    \n",
    "    choices = list(data.choice.astype(int))\n",
    "    rewards = list(np.divide(list(data.reward.astype(int)),98))\n",
    "\n",
    "    num_of_trials = len(choices)\n",
    "    prob_of_choice = [0]*num_of_trials\n",
    "    accuracy = [0]*num_of_trials\n",
    "    \n",
    "    q = list(np.divide([50,50,50,50],98))\n",
    "\n",
    "    q_predictions = []\n",
    "    for trial in range(num_of_trials):\n",
    "        if choices[trial] == -1:\n",
    "            # if it's first trial, there's no previous prediction to count on - make a random choice then. Reward will be 0\n",
    "            if trial == 0:\n",
    "                random_q_ind = random.choice([0,1,2,3])\n",
    "                prob_of_choice[trial] = np.divide( np.exp(np.multiply(beta, q[random_q_ind])),  np.sum(np.exp(np.multiply(beta,q)), axis=0))\n",
    "                accuracy[trial] = int((choices[trial]-1)==random_q_ind)\n",
    "                q_predictions.append(random_q_ind)\n",
    "#                 pe = 0 - q[random_q_ind]\n",
    "#                 q[random_q_ind] = q[random_q_ind] + alpha*pe\n",
    "            else:\n",
    "                prob_of_choice[trial] =  prob_of_choice[trial-1]\n",
    "                accuracy[trial] = accuracy[trial]-1\n",
    "                q_predictions.append(q_predictions[-1])\n",
    "#                 pe = 0 - q[q_predictions[-1]]\n",
    "#                 q[q_predictions[-1]] = q[q_predictions[-1]] + alpha*pe\n",
    "        else:\n",
    "            # regular choice case\n",
    "            prob_of_choice[trial] = np.divide( np.exp(np.multiply(beta, q[choices[trial]-1])),  np.sum(np.exp(np.multiply(beta,q)), axis=0))\n",
    "            accuracy[trial] = int((choices[trial]-1)==np.argmax(q))\n",
    "            q_predictions.append(np.argmax(q))\n",
    "            pe = rewards[trial]-q[choices[trial]-1]\n",
    "            q[choices[trial]-1] = q[choices[trial]-1] + alpha*pe\n",
    "        \n",
    "\n",
    "    return -(np.sum(np.log(prob_of_choice))),accuracy, q_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qlearning_params(person_df, minimum):\n",
    "    bounds = optimize.Bounds([0, 1], [0, 999])\n",
    "    # fmin was ‘Nelder-Mead’ (downhill simplex)\n",
    "    # minimum = optimize.fmin(q_learning_model,x0=(1,0.1),args=(person_df[['choice','reward']],))\n",
    "    # minimize is using ‘L-BFGS-B’\n",
    "    # minimum = optimize.minimize(q_learning_model,x0=(1,0.1),bounds=([0, np.inf], [0, 1]),args=(person_df[['choice','reward', 'user']],))\n",
    "    ll, acc, q_predictions = q_learning_model_fitted(minimum, person_df)\n",
    "    accuracy = np.divide(np.sum(acc),len(acc))\n",
    "    return ll,acc,accuracy, q_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_accuracy(person_df, payoff_person_predictions):\n",
    "    person_choices = person_df.choice -1\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for choice,pred in zip(person_choices,payoff_person_predictions[-1]):\n",
    "        if choice == pred:\n",
    "            correct+=1\n",
    "        if choice>=0:\n",
    "            count+=1\n",
    "    return (correct/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>choice</th>\n",
       "      <th>reward</th>\n",
       "      <th>time</th>\n",
       "      <th>payoff_structure</th>\n",
       "      <th>reward_1</th>\n",
       "      <th>reward_2</th>\n",
       "      <th>reward_3</th>\n",
       "      <th>reward_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>87</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>84</td>\n",
       "      <td>53</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>742.0</td>\n",
       "      <td>2</td>\n",
       "      <td>87</td>\n",
       "      <td>81</td>\n",
       "      <td>50</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>927.0</td>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "      <td>92</td>\n",
       "      <td>61</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  choice  reward    time  payoff_structure  reward_1  reward_2  \\\n",
       "0     1     1.0    84.0  1104.0                 2        84        87   \n",
       "1     1     2.0    90.0  1076.0                 2        90        90   \n",
       "2     1     3.0    53.0   612.0                 2        80        84   \n",
       "3     1     4.0    24.0   742.0                 2        87        81   \n",
       "4     1     2.0    92.0   927.0                 2        86        92   \n",
       "\n",
       "   reward_3  reward_4  \n",
       "0        42        23  \n",
       "1        46        18  \n",
       "2        53        28  \n",
       "3        50        24  \n",
       "4        61        28  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_payoff2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting payoff2\n",
      "starting payoff3\n",
      "starting payoff4\n"
     ]
    }
   ],
   "source": [
    "# instead of running each one seperatly, do one run for all - to get one alpha and beta\n",
    "############ PAYOFF2 #############################################\n",
    "\n",
    "# ll, acc,accuracy, q_predictions = get_qlearning_params(data_payoff2, minimum)\n",
    "#  acc_per_user = get_real_accuracy(data_payoff2)\n",
    "\n",
    "# updated: doing optimization only once, for all\n",
    "minimum = optimize.minimize(q_learning_model,x0=(1,0.1),bounds=([0, np.inf], [0, 1]),args=(data_filled_NA[['choice','reward', 'user']],))\n",
    "\n",
    "\n",
    "print(\"starting payoff2\")\n",
    "############ PAYOFF2 #############################################\n",
    "payoff_2_person_features = []\n",
    "payoff_2_person_accuracies = []    # out of 150\n",
    "payoff_2_person_predictions = []\n",
    "payoff_2_person_real_accuracies = []    # out of choices number\n",
    "\n",
    "# minimum = optimize.minimize(q_learning_model,x0=(1,0.1),bounds=([0, np.inf], [0, 1]),args=(data_payoff2[['choice','reward', 'user']],))\n",
    "\n",
    "for participant in user_ids2:\n",
    "    person_df = data_payoff2[data_payoff2['user']==participant].reset_index(drop=True).copy()\n",
    "    ft_1 = count_stickiness(person_df)\n",
    "    ft_2 = calc_mean_reward(person_df)\n",
    "    ft_3 = minimum.x[0] # beta\n",
    "    ft_4 = minimum.x[1] # alpha\n",
    "    ft_5,acc,accuracy, q_predictions = get_qlearning_params(person_df, minimum)\n",
    "    payoff_2_person_features.append([ft_1,ft_2,ft_3,ft_4,ft_5])\n",
    "    payoff_2_person_accuracies.append(accuracy)\n",
    "    payoff_2_person_predictions.append(q_predictions)\n",
    "\n",
    "    payoff_2_person_real_accuracies.append(get_real_accuracy(person_df, payoff_2_person_predictions))\n",
    "##################################################################\n",
    "print(\"starting payoff3\")\n",
    "############ PAYOFF3 #############################################\n",
    "payoff_3_person_features = []\n",
    "payoff_3_person_accuracies = []    # out of 150\n",
    "payoff_3_person_predictions = []\n",
    "payoff_3_person_real_accuracies = []    # out of choices number\n",
    "\n",
    "# minimum = optimize.minimize(q_learning_model,x0=(1,0.1),bounds=([0, np.inf], [0, 1]),args=(data_payoff3[['choice','reward', 'user']],))\n",
    "\n",
    "for participant in user_ids3:\n",
    "    person_df = data_payoff3[data_payoff3['user']==participant].reset_index(drop=True).copy()\n",
    "    ft_1 = count_stickiness(person_df)\n",
    "    ft_2 = calc_mean_reward(person_df)\n",
    "    ft_3 = minimum.x[0]\n",
    "    ft_4 = minimum.x[1]\n",
    "    ft_5,acc,accuracy, q_predictions = get_qlearning_params(person_df, minimum)\n",
    "    payoff_3_person_features.append([ft_1,ft_2,ft_3,ft_4,ft_5])\n",
    "    payoff_3_person_accuracies.append(accuracy)\n",
    "    payoff_3_person_predictions.append(q_predictions)\n",
    "\n",
    "    payoff_3_person_real_accuracies.append(get_real_accuracy(person_df, payoff_3_person_predictions))\n",
    "##################################################################\n",
    "\n",
    "\n",
    "##################################################################\n",
    "print(\"starting payoff4\")\n",
    "############ PAYOFF4 #############################################\n",
    "payoff_4_person_features = []\n",
    "payoff_4_person_accuracies = []  # out of 150\n",
    "payoff_4_person_predictions = []\n",
    "payoff_4_person_real_accuracies = []  # out of choices number\n",
    "\n",
    "# minimum = optimize.minimize(q_learning_model, x0=(1, 0.1), bounds=([0, np.inf], [0, 1]), args=(data_payoff4[['choice', 'reward', 'user']],))\n",
    "\n",
    "for participant in user_ids4:\n",
    "    person_df = data_payoff4[data_payoff4['user'] == participant].reset_index(drop=True).copy()\n",
    "    ft_1 = count_stickiness(person_df)\n",
    "    ft_2 = calc_mean_reward(person_df)\n",
    "    ft_3 = minimum.x[0]\n",
    "    ft_4 = minimum.x[1]\n",
    "    ft_5, acc, accuracy, q_predictions = get_qlearning_params(person_df, minimum)\n",
    "    payoff_4_person_features.append([ft_1, ft_2, ft_3, ft_4, ft_5])\n",
    "    payoff_4_person_accuracies.append(accuracy)\n",
    "    payoff_4_person_predictions.append(q_predictions)\n",
    "\n",
    "    payoff_4_person_real_accuracies.append(get_real_accuracy(person_df, payoff_4_person_predictions))\n",
    "##################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create true_false_prediction_mat: a indicates if a prediction is correct or wrong, per user per choice. Later is being "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_false_predictions(person_df, participant, payoff_qlearn_person_predictions, payoff_person_real_accuracies, payoff_length_correction):\n",
    "    \"\"\"\n",
    "    gets the person_df and the curresponding user id (participant) with the qlearning predictions (and payoff_person_real_accuracies for assertion), and returns a true/false vector of 150\n",
    "    \"\"\"\n",
    "    person_choices = person_df.choice -1\n",
    "    participant_true_false = []\n",
    "    for choice, prediction in zip(person_choices, payoff_qlearn_person_predictions[participant-1-payoff_length_correction]):\n",
    "        if choice<0:\n",
    "            participant_true_false.append(np.nan)\n",
    "        else:\n",
    "            if choice == prediction:\n",
    "                participant_true_false.append(1)\n",
    "            else:\n",
    "                participant_true_false.append(0)\n",
    "\n",
    "    participant_true_false_np = np.asarray(participant_true_false)\n",
    "    assert(np.nanmean(participant_true_false_np)==payoff_person_real_accuracies[participant-1-payoff_length_correction]),\"accuracy is wrong\"\n",
    "    participant_true_false_np = np.nan_to_num(participant_true_false_np, -1)\n",
    "    return participant_true_false_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QLEARNING MODEL\n",
    "## for each paticipant, get a vector of 150 predictions where each cell is (0 if false, 1 if correct, -1 if blank)\n",
    "############ PAYOFF2 #############################################\n",
    "payoff_2_person_true_false_predictions = []\n",
    "for participant in user_ids2:\n",
    "    person_df = data_payoff2[data_payoff2['user']==participant].reset_index(drop=True).copy()\n",
    "    payoff_2_person_true_false_predictions.append(get_true_false_predictions(person_df, participant, payoff_2_person_predictions, payoff_2_person_real_accuracies, payoff_length_correction=0))\n",
    "\n",
    "payoff_2_person_true_false_predictions_matrix_qlearning = np.asmatrix(payoff_2_person_true_false_predictions)\n",
    "##################################################################\n",
    "\n",
    "\n",
    "############ PAYOFF3 #############################################\n",
    "payoff_3_person_true_false_predictions = []\n",
    "for participant in user_ids3:\n",
    "    person_df = data_payoff3[data_payoff3['user']==participant].reset_index(drop=True).copy()\n",
    "    payoff_3_person_true_false_predictions.append(get_true_false_predictions(person_df, participant, payoff_3_person_predictions, payoff_3_person_real_accuracies, payoff_length_correction=len(user_ids2)))\n",
    "\n",
    "payoff_3_person_true_false_predictions_matrix_qlearning = np.asmatrix(payoff_3_person_true_false_predictions)\n",
    "##################################################################\n",
    "\n",
    "\n",
    "############ PAYOFF4 #############################################\n",
    "payoff_4_person_true_false_predictions = []\n",
    "for participant in user_ids4:\n",
    "    person_df = data_payoff4[data_payoff4['user']==participant].reset_index(drop=True).copy()\n",
    "    payoff_4_person_true_false_predictions.append(get_true_false_predictions(person_df, participant, payoff_4_person_predictions, payoff_4_person_real_accuracies, payoff_length_correction=len(user_ids2)+len(user_ids3)))\n",
    "\n",
    "payoff_4_person_true_false_predictions_matrix_qlearning = np.asmatrix(payoff_4_person_true_false_predictions)\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "payoff2_qlearn_plots_res = payoff_2_person_features, payoff_2_person_accuracies, payoff_2_person_real_accuracies, payoff_2_person_predictions, payoff_2_person_true_false_predictions_matrix_qlearning\n",
    "payoff3_qlearn_plots_res = payoff_3_person_features, payoff_3_person_accuracies, payoff_3_person_real_accuracies, payoff_3_person_predictions, payoff_3_person_true_false_predictions_matrix_qlearning\n",
    "payoff4_qlearn_plots_res = payoff_4_person_features, payoff_4_person_accuracies, payoff_4_person_real_accuracies, payoff_4_person_predictions, payoff_4_person_true_false_predictions_matrix_qlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated: saved on 1.11.21\n",
    "with open(os.path.join(saving_dir,'payoff2_qlearn_reproduce_fixed_paper.pkl'), 'wb') as handle:\n",
    "    pickle.dump(payoff2_qlearn_plots_res, handle)\n",
    "\n",
    "\n",
    "with open(os.path.join(saving_dir,'payoff3_qlearn_reproduce_fixed_paper.pkl'), 'wb') as handle:\n",
    "    pickle.dump(payoff3_qlearn_plots_res, handle)\n",
    "\n",
    "with open(os.path.join(saving_dir,'payoff4_qlearn_reproduce_fixed_paper.pkl'), 'wb') as handle:\n",
    "    pickle.dump(payoff4_qlearn_plots_res, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load for debugging\n",
    "import os\n",
    "saving_dir = '/Users/matanfintz/workspace/thesis_code/Thesis/paper_additions/'\n",
    "with open(os.path.join(saving_dir,'payoff2_qlearn_reproduce_fixed_paper.pkl'), 'rb') as handle:\n",
    "    payoff2_qlearn_plots_res = pickle.load(handle)\n",
    "\n",
    "\n",
    "with open(os.path.join(saving_dir,'payoff3_qlearn_reproduce_fixed_paper.pkl'), 'rb') as handle:\n",
    "    payoff3_qlearn_plots_res = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(saving_dir,'payoff4_qlearn_reproduce_fixed_paper.pkl'), 'rb') as handle:\n",
    "    payoff4_qlearn_plots_res = pickle.load(handle)\n",
    "\n",
    "\n",
    "payoff_2_person_features, payoff_2_person_accuracies, payoff_2_person_real_accuracies, payoff_2_person_predictions, payoff_2_person_true_false_predictions_matrix_qlearning = payoff2_qlearn_plots_res\n",
    "\n",
    "payoff_3_person_features, payoff_3_person_accuracies, payoff_3_person_real_accuracies, payoff_3_person_predictions, payoff_3_person_true_false_predictions_matrix_qlearning = payoff3_qlearn_plots_res\n",
    "\n",
    "payoff_4_person_features, payoff_4_person_accuracies, payoff_4_person_real_accuracies, payoff_4_person_predictions, payoff_4_person_true_false_predictions_matrix_qlearning = payoff4_qlearn_plots_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading_dir = 'explain/qlearning_model_plots/'\n",
    "# with open(os.path.join(loading_dir,'payoff2_qlearn_reproduce_tf.pkl'), 'rb') as handle:\n",
    "#     payoff2_qlearn_plots_res = pickle.load(handle)\n",
    "\n",
    "    \n",
    "# with open(os.path.join(loading_dir,'payoff3_qlearn_reproduce_tf.pkl'), 'rb') as handle:\n",
    "#     payoff3_qlearn_plots_res = pickle.load(handle)\n",
    "\n",
    "# with open(os.path.join(loading_dir,'payoff4_qlearn_reproduce_tf.pkl'), 'rb') as handle:\n",
    "#     payoff4_qlearn_plots_res = pickle.load(handle)\n",
    "\n",
    "    \n",
    "payoff_2_person_features, payoff_2_person_accuracies, payoff_2_person_real_accuracies, payoff_2_person_predictions, payoff_2_person_true_false_predictions_matrix_qlearning = payoff2_qlearn_plots_res\n",
    "payoff_3_person_features, payoff_3_person_accuracies, payoff_3_person_real_accuracies, payoff_3_person_predictions, payoff_3_person_true_false_predictions_matrix_qlearning = payoff3_qlearn_plots_res \n",
    "payoff_4_person_features, payoff_4_person_accuracies, payoff_4_person_real_accuracies, payoff_4_person_predictions, payoff_4_person_true_false_predictions_matrix_qlearning = payoff4_qlearn_plots_res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_acc_per_time(payoff_person_true_false_predictions_matrix):\n",
    "    \"\"\"\n",
    "    gets a payoff_person_true_false_predictions_matrix -  which can be either qlearning or lstm \n",
    "    return a vector of 150, each entrance is the avg accuracy across all participants in that payoff\n",
    "    \"\"\"\n",
    "    payoff_all_pars_avg_acc_per_time = []\n",
    "    for t in range(payoff_person_true_false_predictions_matrix.shape[1]):\n",
    "        collect_not_minuses = []\n",
    "        for pred in payoff_person_true_false_predictions_matrix[:,t]:\n",
    "            if pred<0:\n",
    "                continue\n",
    "            else:\n",
    "                collect_not_minuses.append(pred)\n",
    "        payoff_all_pars_avg_acc_per_time.append(np.mean(collect_not_minuses))\n",
    "    \n",
    "    return payoff_all_pars_avg_acc_per_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_over_time(true_false_predictions_matrix_qlearning, true_false_predictions_matrix_lstm, payoff, legend_1='Qlearning acc', legend_2='LSTM acc', save_fig=False, add_text=\"\"):\n",
    "    \"\"\"\n",
    "    Ploting accuracy comparison over time between 2 models\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(15, 5)\n",
    "    x = np.arange(4,150)\n",
    "    plt.plot(x, get_avg_acc_per_time(true_false_predictions_matrix_qlearning)[4:], 'r') \n",
    "    plt.plot(x, get_avg_acc_per_time(true_false_predictions_matrix_lstm)[4:], 'b') \n",
    "    plt.legend([legend_1, legend_2])\n",
    "    plt.title('Payoff{} accuracy over time Qlearn vs LSTM'.format(payoff), fontdict=my_font_dict)\n",
    "    plt.xlabel('Time', fontdict=my_font_dict)\n",
    "    plt.ylabel('Accuracy', fontdict=my_font_dict)\n",
    "    plt.tick_params(axis='both', labelsize=20)\n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(saving_dir,'payoff{}_accuracy_over_time_{}_Qlearn_vs_LSTM.svg'.format(payoff, add_text)), dpi=200, format='svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_over_time_all_together(true_false_pred_list:list, payoff:int, legend:list, save_fig=False, add_text=\"\"):\n",
    "    \"\"\"\n",
    "    Ploting accuracy comparison over time between (2) models\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(22, 8)\n",
    "    x = np.arange(4,150)\n",
    "    colors = ['r','g','b']\n",
    "    for pred_mat,color in zip(true_false_pred_list, colors):\n",
    "        plt.plot(x, get_avg_acc_per_time(pred_mat)[4:], color) \n",
    "    \n",
    "    plt.legend([l for l in legend])\n",
    "    plt.title('payoff{}_accuracy_over_time_Qlearn_vs_LSTM'.format(payoff))\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('accuracy')\n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(saving_dir,'payoff{}_accuracy_over_time_{}_Qlearn_vs_LSTM.svg'.format(payoff, add_text)), dpi=200, format='svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # divide General lstm true/false matrix to payoffs\n",
    "# payoff2_persons_true_false_predictions_lstm = np.asmatrix(persons_true_false_predictions[0:329])\n",
    "# payoff3_persons_true_false_predictions_lstm = np.asmatrix(persons_true_false_predictions[329:641])\n",
    "# payoff4_persons_true_false_predictions_lstm = np.asmatrix(persons_true_false_predictions[641:965])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matanfintz/opt/anaconda3/envs/ident/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/matanfintz/opt/anaconda3/envs/ident/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-30-9916d8112409>:18: UserWarning: Matplotlib is currently using svg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plot_accuracy_over_time(payoff_2_person_true_false_predictions_matrix_qlearning, general_payoff2_persons_true_false_predictions_lstm, payoff=2, save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matanfintz/opt/anaconda3/envs/ident/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/matanfintz/opt/anaconda3/envs/ident/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-30-9916d8112409>:18: UserWarning: Matplotlib is currently using svg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plot_accuracy_over_time(payoff_3_person_true_false_predictions_matrix_qlearning, general_payoff3_persons_true_false_predictions_lstm, payoff=3, save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matanfintz/opt/anaconda3/envs/ident/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/matanfintz/opt/anaconda3/envs/ident/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-30-9916d8112409>:18: UserWarning: Matplotlib is currently using svg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plot_accuracy_over_time(payoff_4_person_true_false_predictions_matrix_qlearning, general_payoff4_persons_true_false_predictions_lstm, payoff=4, save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lstm_acc_per_person(persons_true_false_predictions):\n",
    "    collect_not_minuses = []\n",
    "    for pred in persons_true_false_predictions:\n",
    "        if pred<0:\n",
    "            continue\n",
    "        else:\n",
    "            collect_not_minuses.append(pred)\n",
    "    return np.mean(collect_not_minuses)\n",
    "#     return np.mean(persons_true_false_predictions[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "payoff2_lstm_accs = []\n",
    "for user_ind in user_ids2:\n",
    "    payoff2_lstm_accs.append(extract_lstm_acc_per_person(general_all_folds_true_false[user_ind-1]))\n",
    "    \n",
    "    \n",
    "payoff3_lstm_accs = []\n",
    "for user_ind in user_ids3:\n",
    "    payoff3_lstm_accs.append(extract_lstm_acc_per_person(general_all_folds_true_false[user_ind-1]))\n",
    "    \n",
    "    \n",
    "payoff4_lstm_accs = []\n",
    "for user_ind in user_ids4:\n",
    "    payoff4_lstm_accs.append(extract_lstm_acc_per_person(general_all_folds_true_false[user_ind-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_all_payoffs(all_dfs, x_axis, y_axis, add_text='', save_fig=False, save_format='svg'):\n",
    "    \"\"\"\n",
    "    scater of participants accuracy of all 3 payoffs with 2 model types. \n",
    "    notice that the order of the dfs passed HAS to be the same order as 2,3,4 .\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    sns.set(rc={'figure.figsize':(12,12)})\n",
    "    comparison_title = '{} VS {}'.format(x_axis, y_axis, fontdict=my_font_dict)\n",
    "    \n",
    "    \n",
    "    ax = sns.scatterplot(x=x_axis, y=y_axis, data=all_dfs,hue='Payoff',legend='full', palette=['r','g','b'])\n",
    "    \n",
    "    plt.title(\"All payoffs participants accuracy comparison {} {}\".format(comparison_title, add_text), fontdict=my_font_dict)\n",
    "    \n",
    "    x1, y1 = [0, 1], [0, 1]\n",
    "#     plt.legend(['payoff_2','payoff_3','payoff_4'])\n",
    "    plt.plot(x1, y1,'k--', marker = 'x')\n",
    "    plt.tick_params(axis='both', labelsize=20)\n",
    "    if save_fig:\n",
    "        if save_format=='svg':\n",
    "            plt.savefig(os.path.join(saving_dir,'payoff_participant_accuracy_comparison_{}.svg'.format(comparison_title.replace(\" \",\"_\"))),quality=95,dpi=200, format='svg')\n",
    "        else:\n",
    "            plt.savefig(os.path.join(saving_dir,'payoff_participant_accuracy_comparison_{}.png'.format(comparison_title.replace(\" \",\"_\"))),quality=95,dpi=200)\n",
    "    plt.show()\n",
    "#     mpl.get_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_vs_ql_p2_acc_df = pd.DataFrame({'Qlearning model':payoff_2_person_accuracies,'LSTM':general_payoff2_lstm_accs})\n",
    "gen_vs_ql_p3_acc_df = pd.DataFrame({'Qlearning model':payoff_3_person_accuracies,'LSTM':general_payoff3_lstm_accs})\n",
    "gen_vs_ql_p4_acc_df = pd.DataFrame({'Qlearning model':payoff_4_person_accuracies,'LSTM':general_payoff4_lstm_accs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all to one df\n",
    "_data=[]\n",
    "for row in gen_vs_ql_p2_acc_df.itertuples():\n",
    "    _data.append({'Qlearning model':row[1],'LSTM':row[2],'Payoff':2})\n",
    "df2 =  pd.DataFrame(_data)\n",
    "\n",
    "_data=[]\n",
    "for row in gen_vs_ql_p3_acc_df.itertuples():\n",
    "    _data.append({'Qlearning model':row[1],'LSTM':row[2],'Payoff':3})\n",
    "df3 =  pd.DataFrame(_data)\n",
    "\n",
    "_data=[]\n",
    "for row in gen_vs_ql_p4_acc_df.itertuples():\n",
    "    _data.append({'Qlearning model':row[1],'LSTM':row[2],'Payoff':4})\n",
    "df4 =  pd.DataFrame(_data)\n",
    "\n",
    "gen_vs_ql_all_dfs= pd.concat([df2,df3,df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-56187883ff6d>:22: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"quality\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
      "  plt.savefig(os.path.join(saving_dir,'payoff_participant_accuracy_comparison_{}.svg'.format(comparison_title.replace(\" \",\"_\"))),quality=95,dpi=200, format='svg')\n",
      "<ipython-input-38-56187883ff6d>:25: UserWarning: Matplotlib is currently using svg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# create_scatter_all_payoffs(dfs=[gen_vs_ql_p2_acc_df,gen_vs_ql_p3_acc_df,gen_vs_ql_p4_acc_df], x_axis='Qlearning_Model', y_axis='LSTM' ,save_fig=True)\n",
    "create_scatter_all_payoffs(gen_vs_ql_all_dfs,x_axis='Qlearning model', y_axis='LSTM' ,save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# summary results\n",
    "# pd.DataFrame({'payoff2':[np.mean(payoff_2_person_real_accuracies)],\n",
    "#               'payoff3':[np.mean(payoff_3_person_real_accuracies)],\n",
    "#               'payoff4':[np.mean(payoff_4_person_real_accuracies)],\n",
    "#               'full_avg':q_leanr_avg\n",
    "#               },\n",
    "#              index=['q_learn_model'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ident",
   "language": "python",
   "name": "ident"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
